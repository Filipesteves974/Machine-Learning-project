{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5487d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports that we will need will almost always use - it is a good practice to import all libraries at the beginning of the notebook or script\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "# data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#filter methods\n",
    "# spearman \n",
    "# chi-square\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "#wrapper methods\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import lasso_path, SGDRegressor\n",
    "\n",
    "\n",
    "# embedded methods\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.preprocessing import TargetEncoder, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, root_mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "\n",
    "#set random seed for reproducibility\n",
    "RSEED = 42\n",
    "np.random.seed(RSEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036fc4e",
   "metadata": {},
   "source": [
    "Transforms all data to fit the same criteria making it easier to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "816a2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x):\n",
    "    x = str(x)\n",
    "    x = x.lower()\n",
    "    x = x.replace(\"_\", \"\")\n",
    "    x = x.replace(\"-\", \"\")\n",
    "    x = x.replace(\" \", \"\")\n",
    "    if x == \"nan\":\n",
    "        return np.nan\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09780e",
   "metadata": {},
   "source": [
    "Function that automatically looks for the closest match on the valid list, therefore correcting the visible typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db10d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_missing_letters(value, valid_list, max_missing=2):\n",
    "    \"\"\"\n",
    "    corrects values with missing letters based on valid_list\n",
    "    \"\"\"\n",
    "    best_match = value\n",
    "    smallest_diff = 999\n",
    "    if pd.isna(value):  # <- ignores NaN\n",
    "        return np.nan\n",
    "    for ref in valid_list:\n",
    "        # absolute length difference\n",
    "        len_diff = abs(len(ref) - len(value))\n",
    "        if len_diff == 0 or len_diff > max_missing:\n",
    "            continue  # ignora se igual ou diferença > limite\n",
    "\n",
    "        # verificar se o valor é subsequência do nome correto (mantendo ordem)\n",
    "        it = iter(ref)\n",
    "        is_subseq = all(ch in it for ch in value)\n",
    "\n",
    "        if is_subseq and len_diff < smallest_diff:\n",
    "            smallest_diff = len_diff\n",
    "            best_match = ref\n",
    "\n",
    "    return best_match\n",
    "\n",
    "valid_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7477024",
   "metadata": {},
   "source": [
    "The remove_outliers_smart_v3 function handles abnormal or extreme values in car data to prepare training, validation, and test sets for a machine learning model. It does this without removing any rows from the data, only changing the values within the columns where they are problematic. The treatment follows two main logics. The first is replacement with NaN for impossible or illogical values, for example, years of manufacture prior to 1990, engines with a displacement greater than 6 litres, and also strange combinations such as very new cars with excessively high mileage or large engines with unrealistically low fuel consumption. The problematic value in that cell is replaced with NaN, indicating that it is missing and will be handled later. The second logic is capping or limitation, which is applied to columns such as mileage, mpg consumption, and tax. In this case, instead of replacing values that are above a certain high percentile, such as 98% or 99%, with NaN calculated in the training set, they are cut off and replaced by this upper limit. For mpg, a cut-off is also made at the lower limit to avoid values close to zero or negative. This approach of cutting or replacing with NaN instead of removing the entire row ensures that the size of your training, validation, and test datasets remains the same, but with much cleaner and more consistent data, which is crucial for building a quality model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18d482ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(X_train, X_val, X_test, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Trata outliers:\n",
    "    - Train/Val/Test: Substitui valores impossíveis por NaN (não remove linhas)\n",
    "    - Train/Val/Test: Faz capping em valores extremos\n",
    "    \"\"\"\n",
    "    X_tr = X_train.copy()\n",
    "    X_v = X_val.copy()\n",
    "    X_tst = X_test.copy()\n",
    "    y_tr = y_train.copy()\n",
    "    y_v = y_val.copy()\n",
    "    \n",
    "    \n",
    "    # ========== YEAR < 1990 ==========\n",
    "    if 'year' in X_tr.columns:\n",
    "        mask_tr = X_tr['year'] < 1990\n",
    "        mask_v = X_v['year'] < 1990\n",
    "        mask_tst = X_tst['year'] < 1990\n",
    "        \n",
    "        removed_tr = mask_tr.sum()\n",
    "        removed_v = mask_v.sum()\n",
    "        removed_tst = mask_tst.sum()\n",
    "        \n",
    "        X_tr.loc[mask_tr, 'year'] = np.nan\n",
    "        X_v.loc[mask_v, 'year'] = np.nan\n",
    "        X_tst.loc[mask_tst, 'year'] = np.nan\n",
    "        \n",
    "        if removed_tr > 0 or removed_v > 0 or removed_tst > 0:\n",
    "            print(f\"\\n[YEAR < 1990]\")\n",
    "            print(f\" {removed_tr} train, {removed_v} val, {removed_tst} test (→ NaN)\")\n",
    "    \n",
    "\n",
    "    # ========== MILEAGE (Capping) ==========\n",
    "    if 'mileage' in X_tr.columns:\n",
    "        print(f\"\\n[MILEAGE]\")\n",
    "        \n",
    "        upper_mileage = X_train['mileage'].quantile(0.98)\n",
    "        train_above = (X_tr['mileage'] > upper_mileage).sum()\n",
    "        val_above = (X_v['mileage'] > upper_mileage).sum()\n",
    "        test_above = (X_tst['mileage'] > upper_mileage).sum()\n",
    "        \n",
    "        print(f\" P99 = {upper_mileage:,.0f} milhas\")\n",
    "        print(f\" Capped: {train_above} train, {val_above} val, {test_above} test\")\n",
    "        \n",
    "        X_tr['mileage'] = np.clip(X_tr['mileage'], 0, upper_mileage)\n",
    "        X_v['mileage'] = np.clip(X_v['mileage'], 0, upper_mileage)\n",
    "        X_tst['mileage'] = np.clip(X_tst['mileage'], 0, upper_mileage)\n",
    "    \n",
    "\n",
    "    # ========== MPG (Capping) ==========\n",
    "    if 'mpg' in X_tr.columns:\n",
    "        print(f\"\\n[MPG]\")\n",
    "        q_low = X_tr['mpg'].quantile(0.005)\n",
    "        q_high = X_tr['mpg'].quantile(0.98)\n",
    "        print(f\" [{q_low:.1f}, {q_high:.1f}] MPG (0.5%–98%)\")\n",
    "        \n",
    "        train_affected = ((X_tr['mpg'] < q_low) | (X_tr['mpg'] > q_high)).sum()\n",
    "        val_affected = ((X_v['mpg'] < q_low) | (X_v['mpg'] > q_high)).sum()\n",
    "        test_affected = ((X_tst['mpg'] < q_low) | (X_tst['mpg'] > q_high)).sum()\n",
    "        \n",
    "        print(f\"  {train_affected} train, {val_affected} val, {test_affected} test\")\n",
    "        \n",
    "        X_tr['mpg'] = np.clip(X_tr['mpg'], q_low, q_high)\n",
    "        X_v['mpg'] = np.clip(X_v['mpg'], q_low, q_high)\n",
    "        X_tst['mpg'] = np.clip(X_tst['mpg'], q_low, q_high)\n",
    "    \n",
    "\n",
    "    # ========== TAX (Capping) ==========\n",
    "    if 'tax' in X_tr.columns:\n",
    "        print(f\"\\n[TAX]\")\n",
    "        upper_tax = X_train['tax'].quantile(0.98)\n",
    "        train_above = (X_tr['tax'] > upper_tax).sum()\n",
    "        val_above = (X_v['tax'] > upper_tax).sum()\n",
    "        test_above = (X_tst['tax'] > upper_tax).sum()\n",
    "        \n",
    "        print(f\"  P98 = £{upper_tax:.0f}\")\n",
    "        print(f\"  Capped: {train_above} train, {val_above} val, {test_above} test\")\n",
    "        \n",
    "        X_tr['tax'] = np.clip(X_tr['tax'], 0, upper_tax)\n",
    "        X_v['tax'] = np.clip(X_v['tax'], 0, upper_tax)\n",
    "        X_tst['tax'] = np.clip(X_tst['tax'], 0, upper_tax)\n",
    "    \n",
    "\n",
    "    # ========== ENGINE SIZE ==========\n",
    "    if 'engineSize' in X_tr.columns:\n",
    "        print(f\"\\n[ENGINE SIZE]\")\n",
    "        \n",
    "        mask_tr = ((X_tr['engineSize'] > 6.0) | (X_tr['engineSize'] < 0.5))\n",
    "        mask_v  = ((X_v['engineSize'] > 6.0)  | (X_v['engineSize'] < 0.5))\n",
    "        mask_tst = ((X_tst['engineSize'] > 6.0) | (X_tst['engineSize'] < 0.5))\n",
    "        \n",
    "        removed_tr = mask_tr.sum()\n",
    "        removed_v = mask_v.sum()\n",
    "        removed_tst = mask_tst.sum()\n",
    "        \n",
    "        X_tr.loc[mask_tr, 'engineSize'] = np.nan\n",
    "        X_v.loc[mask_v, 'engineSize'] = np.nan\n",
    "        X_tst.loc[mask_tst, 'engineSize'] = np.nan\n",
    "        \n",
    "        if removed_tr > 0 or removed_v > 0 or removed_tst > 0:\n",
    "            print(f\" Engine > 6.0L: {removed_tr} train, {removed_v} val, {removed_tst} test (→ NaN)\")\n",
    "    \n",
    "\n",
    "    # ========== LOGIC VALIDATION ==========\n",
    "    print(f\"\\n[Logic Validation]\")\n",
    "    \n",
    "    # new cars with high mileage (physically impossible)\n",
    "    if 'year' in X_tr.columns and 'mileage' in X_tr.columns:\n",
    "        current_year = 2025\n",
    "        \n",
    "        mask_tr = (current_year - X_tr['year'] <= 3) & (X_tr['mileage'] > 100000)\n",
    "        mask_v = (current_year - X_v['year'] <= 3) & (X_v['mileage'] > 100000)\n",
    "        mask_tst = (current_year - X_tst['year'] <= 3) & (X_tst['mileage'] > 100000)\n",
    "        \n",
    "        removed_tr = mask_tr.sum()\n",
    "        removed_v = mask_v.sum()\n",
    "        removed_tst = mask_tst.sum()\n",
    "        \n",
    "        X_tr.loc[mask_tr, 'year'] = np.nan\n",
    "        X_v.loc[mask_v, 'year'] = np.nan\n",
    "        X_tst.loc[mask_tst, 'year'] = np.nan\n",
    "        \n",
    "        if removed_tr > 0 or removed_v > 0 or removed_tst > 0:\n",
    "            print(f\" New cars + high km: {removed_tr} train, {removed_v} val, {removed_tst} test (→ NaN)\")\n",
    "    \n",
    "    # large engine with high MPG (physically improbable)\n",
    "    if 'mpg' in X_tr.columns and 'engineSize' in X_tr.columns:\n",
    "        mask_tr = (X_tr['engineSize'] > 4.0) & (X_tr['mpg'] > 60)\n",
    "        mask_v = (X_v['engineSize'] > 4.0) & (X_v['mpg'] > 60)\n",
    "        mask_tst = (X_tst['engineSize'] > 4.0) & (X_tst['mpg'] > 60)\n",
    "        \n",
    "        removed_tr = mask_tr.sum()\n",
    "        removed_v = mask_v.sum()\n",
    "        removed_tst = mask_tst.sum()\n",
    "        \n",
    "        X_tr.loc[mask_tr, 'mpg'] = np.nan\n",
    "        X_v.loc[mask_v, 'mpg'] = np.nan\n",
    "        X_tst.loc[mask_tst, 'mpg'] = np.nan\n",
    "        \n",
    "        if removed_tr > 0 or removed_v > 0 or removed_tst > 0:\n",
    "            print(f\" large engine + high mpg: {removed_tr} train, {removed_v} val, {removed_tst} test (→ NaN)\")\n",
    "    \n",
    "\n",
    "    # ========== SUMMARY ==========\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Mantidos: {len(X_tr)} train (100.0%), \"\n",
    "          f\"{len(X_v)} val (100.0%), \"\n",
    "          f\"{len(X_tst)} test (100.0%)\")\n",
    "    print(f\"Nenhuma linha removida - valores impossíveis substituídos por NaN\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return X_tr, X_v, X_tst, y_tr, y_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb3604",
   "metadata": {},
   "source": [
    "Hybrid solution for filling in the missing values based on their statistical peers.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The pipeline begins with high-cardinality categorical variables. In the case of `model`, imputation is performed using global mode, since the categories are numerous and there is insufficient information for reliable conditional imputation. For `Brand`, the function takes a smarter approach: it first learns a model→brand mapping from the available data, and only then uses external reference lists to identify the correct brand when the model does not exist in the training sample. This step avoids systematic errors, such as associating an ‘Astra’ with BMW, and produces semantically consistent values. When no rule can resolve the missing value, the global mode is applied as a last resort.\n",
    "\n",
    "This is followed by the imputation of conditional categorical variables, namely `fuelType` and `transmission`. Here, the function calculates the mode by logically relevant groups: for `fuelType` it uses the `brand`, and for `transmission` it uses the combination (`Brand`, `fuelType`), then resorts to the fallback for the mode by brand and finally to the global mode. This strategy respects actual patterns in the automotive market — for example, the fact that certain brands and fuel types tend to have consistent transmissions — avoiding random or structurally inconsistent imputations.\n",
    "\n",
    "Binary variables, such as `has_reported_damage`, are imputed by mode, which is appropriate for attributes with only two possible states.\n",
    "Before proceeding to numerical imputation, the function ensures that validation and testing do not introduce new categories that do not exist in the training data. Unknown categorical values are replaced by the mode of the respective column.\n",
    "\n",
    "The most sophisticated step in the pipeline handles numeric variables with missing values using Iterative Imputer (MICE) with a RandomForestRegressor as the estimator. This method models each numeric variable with missing values as a function of the others, iteratively, capturing non-linear relationships and preserving important correlations between attributes — such as the relationship between vehicle age, mileage, engine size, or fuel consumption. In order for MICE to integrate categorical variables, these are temporarily converted into numerical codes consistent with the training values, ensuring consistency. After imputation, only the values of the numerical columns are replaced, keeping the original categories intact.\n",
    "\n",
    "Finally, the function applies plausibility validations (‘clipping’). These limits ensure that the imputed values remain within the physically possible or commercially realistic range — for example, years between 1990 and 2025, engine capacity between 0.5L and 10L, mileage not negative and below the expected upper limit. This prevents subsequent models from dealing with absurd values or artefacts produced by iterative imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5935b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def impute_missing_values_hybrid(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Hybrid intelligent imputation:\n",
    "    1. Simple categorical: model, Brand (rules + mode)\n",
    "    2. Conditional categorical: fuelType, transmission (mode by group)\n",
    "    3. Binary flags: has_damage, has_reported_damage (mode)\n",
    "    4. Correlated numerical: IterativeImputer (MICE)\n",
    "    5. Optional flags to indicate imputed values\n",
    "    \"\"\"\n",
    "    X_tr = X_train.copy()\n",
    "    X_v = X_val.copy()\n",
    "    X_te = X_test.copy()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"HYBRID IMPUTATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    \n",
    "# =========================================================================\n",
    "    # STEP 1: MODEL (brand mode if Brand known, else global mode)\n",
    "    # =========================================================================\n",
    "    print(\"\\n[1/6] MODEL - brand-aware mode + global fallback\")\n",
    "\n",
    "    \n",
    "    global_mode_model = X_tr[\"model\"].mode()[0] if len(X_tr[\"model\"].mode()) > 0 else \"unknown\"\n",
    "\n",
    "   \n",
    "    n_missing_train = X_tr[\"model\"].isna().sum()\n",
    "\n",
    "    \n",
    "    brand_to_model_mode = (\n",
    "        X_tr.dropna(subset=[\"Brand\", \"model\"])\n",
    "            .groupby(\"Brand\")[\"model\"]\n",
    "            .agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else None)\n",
    "            .to_dict()\n",
    "    )\n",
    "\n",
    "    def fill_model(df):\n",
    "        \n",
    "        miss_model = df[\"model\"].isna()\n",
    "\n",
    "        \n",
    "        has_brand = df[\"Brand\"].notna()\n",
    "        idx_brand = df.index[miss_model & has_brand]\n",
    "        df.loc[idx_brand, \"model\"] = df.loc[idx_brand, \"Brand\"].map(brand_to_model_mode)\n",
    "\n",
    "      \n",
    "        df[\"model\"] = df[\"model\"].fillna(global_mode_model)\n",
    "\n",
    "    \n",
    "    fill_model(X_tr)\n",
    "    fill_model(X_v)\n",
    "    fill_model(X_te)\n",
    "\n",
    "    print(f\"  Global mode: '{global_mode_model}'\")\n",
    "    print(f\"  Imputed - Train: {n_missing_train}, Val: {X_val['model'].isna().sum()}, \"\n",
    "        f\"Test: {X_test['model'].isna().sum()}\")\n",
    "    # =========================================================================\n",
    "    # STEP 2: BRAND (inferred from model, then mode)\n",
    "    # =========================================================================\n",
    "    print(\"\\n[2/6] BRAND - inferred from model + learned mapping\")\n",
    "    \n",
    "    # Create model->Brand dictionary from known data\n",
    "    model_to_brand_map = (\n",
    "        X_tr.dropna(subset=['Brand', 'model'])\n",
    "        .groupby('model')['Brand']\n",
    "        .agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else None)\n",
    "        .to_dict()\n",
    "    )\n",
    "    \n",
    "    # Fallback: hardcoded lists for cases not in data\n",
    "    toyota = [\"yaris\", \"aygo\", \"corolla\", \"chr\", \"avensis\", \"prius\", \"rav4\", \"hilux\", \n",
    "              \"verso\", \"supra\", \"landcruiser\", \"camry\", \"proaceverso\", \"urbancruiser\", \n",
    "              \"auris\", \"gt86\"]\n",
    "    ford = [\"focus\", \"fiesta\", \"mondeo\", \"kuga\", \"galaxy\", \"smax\", \"bmax\", \"ecosport\", \n",
    "            \"puma\", \"tourneocustom\", \"tourneoconnect\", \"grandtourneoconnect\", \"cmax\", \n",
    "            \"grandcmax\", \"edge\", \"mustang\", \"fusion\", \"streetka\", \"ranger\", \"escort\", \n",
    "            \"ka\", \"ka+\"]\n",
    "    opel = [\"corsa\", \"mokkax\", \"astra\", \"insignia\", \"mokka\", \"zafira\", \"viva\", \"meriva\", \n",
    "            \"adam\", \"combolife\", \"crosslandx\", \"grandlandx\", \"gtc\", \"antara\", \"vivaro\", \n",
    "            \"vectra\", \"agila\", \"tigra\", \"cascada\", \"ampera\"]\n",
    "    vw = [\"golf\", \"golfsv\", \"polo\", \"passat\", \"tiguan\", \"tiguanallspace\", \"touran\", \n",
    "          \"touareg\", \"troc\", \"tcross\", \"arteon\", \"sharan\", \"jetta\", \"cc\", \"caravelle\", \n",
    "          \"california\", \"caddy\", \"caddymaxi\", \"beetle\", \"scirocco\", \"up\", \"amarok\", \"eos\", \"fox\"]\n",
    "    audi = [\"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\", \"a7\", \"a8\", \"q2\", \"q3\", \"q5\", \"q7\", \n",
    "            \"q8\", \"s3\", \"s4\", \"s5\", \"s8\", \"rs3\", \"rs4\", \"rs5\", \"rs6\", \"sq5\", \"sq7\", \"tt\", \"r8\"]\n",
    "    mercedes = [\"aclass\", \"bclass\", \"cclass\", \"eclass\", \"sclass\", \"claclass\", \"clsclass\", \n",
    "                \"glaclass\", \"glbclass\", \"glcclass\", \"gleclass\", \"glsclass\", \"glclass\", \n",
    "                \"gclass\", \"vclass\", \"xclass\", \"slclass\", \"slkclass\", \"mclass\", \"slc\", \n",
    "                \"clk\", \"clclass\", \"clcclass\", \"mercedes200\", \"mercedes220\", \"mercedes230\"]\n",
    "    skoda = [\"fabia\", \"octavia\", \"superb\", \"karoq\", \"kodiaq\", \"kamiq\", \"yeti\", \n",
    "             \"yetioutdoor\", \"scala\", \"rapid\", \"citigo\", \"roomster\"]\n",
    "    hyundai = [\"i10\", \"i20\", \"i30\", \"i40\", \"i800\", \"ioniq\", \"kona\", \"tucson\", \"santafe\", \n",
    "               \"getz\", \"ix20\", \"ix35\", \"veloster\", \"accent\", \"terracan\"]\n",
    "    bmw_models = [\"series1\", \"series2\", \"series3\", \"series4\", \"series5\", \"series6\", \n",
    "                  \"series7\", \"series8\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \n",
    "                  \"z3\", \"z4\", \"m2\", \"m3\", \"m4\", \"m5\", \"m6\", \"iq\"]\n",
    "    seat_models = [\"leon\", \"ateca\", \"toledo\", \"arona\", \"ibiza\", \"alhambra\"]\n",
    "    \n",
    "    def infer_brand_smart(model_val):\n",
    "        if pd.isna(model_val):\n",
    "            return None\n",
    "        \n",
    "        # First try learned mapping\n",
    "        if model_val in model_to_brand_map:\n",
    "            return model_to_brand_map[model_val]\n",
    "        \n",
    "        # Fallback to hardcoded lists\n",
    "        m = str(model_val).lower()\n",
    "        if m in toyota: return \"toyota\"\n",
    "        if m in ford: return \"ford\"\n",
    "        if m in opel: return \"opel\"\n",
    "        if m in vw: return \"vw\"\n",
    "        if m in audi: return \"audi\"\n",
    "        if m in bmw_models: return \"bmw\"\n",
    "        if m in mercedes: return \"mercedes\"\n",
    "        if m in skoda: return \"skoda\"\n",
    "        if m in hyundai: return \"hyundai\"\n",
    "        if m in seat_models: return \"seat\"\n",
    "        if m == \"kadjar\": return \"renault\"\n",
    "        if m == \"shuttle\": return \"honda\"\n",
    "        return None\n",
    "    \n",
    "    # Apply inference\n",
    "    n_missing_brand = X_tr[\"Brand\"].isna().sum()\n",
    "    for df in [X_tr, X_v, X_te]:\n",
    "        mask_nan = df[\"Brand\"].isna()\n",
    "        df.loc[mask_nan, \"Brand\"] = df.loc[mask_nan, \"model\"].apply(infer_brand_smart)\n",
    "    \n",
    "    # Global mode for remaining\n",
    "    global_mode_brand = X_tr[\"Brand\"].mode()[0] if len(X_tr[\"Brand\"].mode()) > 0 else \"ford\"\n",
    "    X_tr[\"Brand\"].fillna(global_mode_brand, inplace=True)\n",
    "    X_v[\"Brand\"].fillna(global_mode_brand, inplace=True)\n",
    "    X_te[\"Brand\"].fillna(global_mode_brand, inplace=True)\n",
    "    \n",
    "    print(f\"  Learned mapping: {len(model_to_brand_map)} models\")\n",
    "    print(f\"  Imputed - Train: {n_missing_brand}, Val: {X_val['Brand'].isna().sum()}, \"\n",
    "          f\"Test: {X_test['Brand'].isna().sum()}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: CONDITIONAL CATEGORICAL (fuelType, transmission)\n",
    "    # =========================================================================\n",
    "    print(\"\\n[3/6] FUELTYPE & TRANSMISSION - mode by group\")\n",
    "    \n",
    "    # fuelType by Brand\n",
    "    mode_fueltype_brand = (\n",
    "        X_tr.groupby(\"Brand\")[\"fuelType\"]\n",
    "        .apply(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "    )\n",
    "    global_mode_fueltype = X_tr[\"fuelType\"].mode()[0] if len(X_tr[\"fuelType\"].mode()) > 0 else \"Petrol\"\n",
    "    \n",
    "    def fill_fueltype(row):\n",
    "        if pd.notna(row[\"fuelType\"]):\n",
    "            return row[\"fuelType\"]\n",
    "        val = mode_fueltype_brand.get(row[\"Brand\"], global_mode_fueltype)\n",
    "        return val if pd.notna(val) else global_mode_fueltype\n",
    "    \n",
    "    n_missing_fuel = X_tr[\"fuelType\"].isna().sum()\n",
    "    X_tr[\"fuelType\"] = X_tr.apply(fill_fueltype, axis=1)\n",
    "    X_v[\"fuelType\"] = X_v.apply(fill_fueltype, axis=1)\n",
    "    X_te[\"fuelType\"] = X_te.apply(fill_fueltype, axis=1)\n",
    "    \n",
    "    # transmission by Brand + fuelType\n",
    "    mode_transmission_brandfuel = (\n",
    "        X_tr.groupby([\"Brand\", \"fuelType\"])[\"transmission\"]\n",
    "        .apply(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "    )\n",
    "    mode_transmission_brand = (\n",
    "        X_tr.groupby(\"Brand\")[\"transmission\"]\n",
    "        .apply(lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan)\n",
    "    )\n",
    "    global_mode_transmission = X_tr[\"transmission\"].mode()[0] if len(X_tr[\"transmission\"].mode()) > 0 else \"Manual\"\n",
    "    \n",
    "    def fill_transmission(row):\n",
    "        if pd.notna(row[\"transmission\"]):\n",
    "            return row[\"transmission\"]\n",
    "        val = mode_transmission_brandfuel.get((row[\"Brand\"], row[\"fuelType\"]))\n",
    "        if pd.isna(val):\n",
    "            val = mode_transmission_brand.get(row[\"Brand\"], global_mode_transmission)\n",
    "        return val if pd.notna(val) else global_mode_transmission\n",
    "    \n",
    "    n_missing_trans = X_tr[\"transmission\"].isna().sum()\n",
    "    X_tr[\"transmission\"] = X_tr.apply(fill_transmission, axis=1)\n",
    "    X_v[\"transmission\"] = X_v.apply(fill_transmission, axis=1)\n",
    "    X_te[\"transmission\"] = X_te.apply(fill_transmission, axis=1)\n",
    "    \n",
    "    print(f\"  fuelType imputed - Train: {n_missing_fuel}\")\n",
    "    print(f\"  transmission imputed - Train: {n_missing_trans}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3.5: BINARY FLAGS (has_damage, has_reported_damage)\n",
    "    # =========================================================================\n",
    "    print(\"\\n[3.5/6] BINARY FLAGS - has_reported_damage\")\n",
    "    \n",
    "    for col in ['has_reported_damage']:\n",
    "        if col in X_tr.columns:\n",
    "            mode_val = X_tr[col].mode()[0] if len(X_tr[col].mode()) > 0 else 0\n",
    "            n_missing_train = X_tr[col].isna().sum()\n",
    "            n_missing_val = X_v[col].isna().sum()\n",
    "            n_missing_test = X_te[col].isna().sum()\n",
    "            \n",
    "            X_tr[col].fillna(mode_val, inplace=True)\n",
    "            X_v[col].fillna(mode_val, inplace=True)\n",
    "            X_te[col].fillna(mode_val, inplace=True)\n",
    "            \n",
    "            if n_missing_train > 0 or n_missing_val > 0 or n_missing_test > 0:\n",
    "                print(f\"  {col} - mode: {mode_val}, imputed Train: {n_missing_train}, \"\n",
    "                      f\"Val: {n_missing_val}, Test: {n_missing_test}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: ENSURE KNOWN CATEGORICAL VALUES (before MICE)\n",
    "    # =========================================================================\n",
    "    print(\"\\n[4/6] SYNCHRONIZATION - force known categorical values\")\n",
    "    \n",
    "    cat_cols_to_sync = ['Brand', 'model', 'fuelType', 'transmission']\n",
    "    \n",
    "    for col in cat_cols_to_sync:\n",
    "        if col in X_tr.columns:\n",
    "            # Get known values (excluding NaN)\n",
    "            known_values = set(X_tr[col].dropna().unique())\n",
    "            mode_val = X_tr[col].mode()[0]\n",
    "            \n",
    "            # Val: replace unknown with mode (only non-null values)\n",
    "            mask_unknown_val = X_v[col].notna() & (~X_v[col].isin(known_values))\n",
    "            n_unknown_val = mask_unknown_val.sum()\n",
    "            if n_unknown_val > 0:\n",
    "                X_v.loc[mask_unknown_val, col] = mode_val\n",
    "                print(f\"  {col} - Val: {n_unknown_val} unknown values -> '{mode_val}'\")\n",
    "            \n",
    "            # Test: same\n",
    "            mask_unknown_test = X_te[col].notna() & (~X_te[col].isin(known_values))\n",
    "            n_unknown_test = mask_unknown_test.sum()\n",
    "            if n_unknown_test > 0:\n",
    "                X_te.loc[mask_unknown_test, col] = mode_val\n",
    "                print(f\"  {col} - Test: {n_unknown_test} unknown values -> '{mode_val}'\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 5: CORRELATED NUMERICAL - IterativeImputer (MICE)\n",
    "    # =========================================================================\n",
    "    print(\"\\n[5/6] NUMERICAL - IterativeImputer (MICE)\")\n",
    "    \n",
    "    numeric_cols = ['year', 'engineSize', 'mileage', 'mpg', 'tax', 'previousOwners']\n",
    "    if 'paintQuality%' in X_tr.columns:\n",
    "        numeric_cols.append('paintQuality%')\n",
    "    \n",
    "    # Check which have missing\n",
    "    numeric_cols_with_missing = [col for col in numeric_cols \n",
    "                                  if X_tr[col].isna().sum() > 0]\n",
    "    \n",
    "    if numeric_cols_with_missing:\n",
    "        print(f\"  Columns to impute: {numeric_cols_with_missing}\")\n",
    "        \n",
    "        # Prepare data for imputer\n",
    "        # Convert categorical to numeric codes temporarily\n",
    "        cat_cols = ['Brand', 'model', 'fuelType', 'transmission']\n",
    "        \n",
    "        # Create temporary copies\n",
    "        X_tr_temp = X_tr.copy()\n",
    "        X_v_temp = X_v.copy()\n",
    "        X_te_temp = X_te.copy()\n",
    "        \n",
    "        # Temporary label encoding\n",
    "        label_mappings = {}\n",
    "        for col in cat_cols:\n",
    "            if col in X_tr_temp.columns:\n",
    "                # Create mapping from train (excluding NaN)\n",
    "                unique_vals = X_tr_temp[col].dropna().unique()\n",
    "                mapping = {val: idx for idx, val in enumerate(unique_vals)}\n",
    "                label_mappings[col] = mapping\n",
    "                \n",
    "                # Apply mapping (unknown values remain as NaN)\n",
    "                X_tr_temp[col] = X_tr_temp[col].map(mapping)\n",
    "                X_v_temp[col] = X_v_temp[col].map(mapping)\n",
    "                X_te_temp[col] = X_te_temp[col].map(mapping)\n",
    "        \n",
    "        # Select features for imputer\n",
    "        features_for_imputation = cat_cols + numeric_cols\n",
    "        features_for_imputation = [f for f in features_for_imputation if f in X_tr_temp.columns]\n",
    "        \n",
    "        # Configure and train imputer\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=RandomForestRegressor(n_estimators=10, max_depth=10, random_state=42),\n",
    "            max_iter=10,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit on train\n",
    "        X_tr_imputed = imputer.fit_transform(X_tr_temp[features_for_imputation])\n",
    "        X_v_imputed = imputer.transform(X_v_temp[features_for_imputation])\n",
    "        X_te_imputed = imputer.transform(X_te_temp[features_for_imputation])\n",
    "        \n",
    "        # Replace only imputed numerical columns\n",
    "        for i, col in enumerate(numeric_cols):\n",
    "            if col in features_for_imputation:\n",
    "                idx = features_for_imputation.index(col)\n",
    "                X_tr[col] = X_tr_imputed[:, idx]\n",
    "                X_v[col] = X_v_imputed[:, idx]\n",
    "                X_te[col] = X_te_imputed[:, idx]\n",
    "        \n",
    "        print(f\"  IterativeImputer applied successfully\")\n",
    "    else:\n",
    "        print(f\"  No numerical columns with missing values\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 6: VALIDATION AND CORRECTIONS\n",
    "    # =========================================================================\n",
    "    print(\"\\n[6/6] VALIDATION - checking logical limits\")\n",
    "    \n",
    "    # Sanity corrections\n",
    "    if 'year' in X_tr.columns:\n",
    "        for df in [X_tr, X_v, X_te]:\n",
    "            df['year'] = df['year'].clip(lower=1990, upper=2025)\n",
    "    \n",
    "    if 'engineSize' in X_tr.columns:\n",
    "        for df in [X_tr, X_v, X_te]:\n",
    "            df['engineSize'] = df['engineSize'].clip(lower=0.5, upper=10.0)\n",
    "    \n",
    "    if 'mileage' in X_tr.columns:\n",
    "        for df in [X_tr, X_v, X_te]:\n",
    "            df['mileage'] = df['mileage'].clip(lower=0, upper=500000)\n",
    "    \n",
    "    if 'mpg' in X_tr.columns:\n",
    "        for df in [X_tr, X_v, X_te]:\n",
    "            df['mpg'] = df['mpg'].clip(lower=10, upper=200)\n",
    "    \n",
    "    if 'tax' in X_tr.columns:\n",
    "        for df in [X_tr, X_v, X_te]:\n",
    "            df['tax'] = df['tax'].clip(lower=0, upper=1000)\n",
    "    \n",
    "    if 'previousOwners' in X_tr.columns:\n",
    "        for df in [X_tr, X_v, X_te]:\n",
    "            df['previousOwners'] = df['previousOwners'].clip(lower=0, upper=10).round()\n",
    "    \n",
    "    if 'paintQuality%' in X_tr.columns:\n",
    "        for df in [X_tr, X_v, X_te]:\n",
    "            df['paintQuality%'] = df['paintQuality%'].clip(lower=0, upper=100)\n",
    "    \n",
    "    print(f\"  Limits applied\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FINAL REPORT\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"IMPUTATION COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nFinal missing values:\")\n",
    "    print(f\"  Train: {X_tr.isna().sum().sum()}\")\n",
    "    print(f\"  Val:   {X_v.isna().sum().sum()}\")\n",
    "    print(f\"  Test:  {X_te.isna().sum().sum()}\")\n",
    "    \n",
    "    if X_tr.isna().sum().sum() > 0:\n",
    "        print(\"\\nColumns with remaining NaNs in Train:\")\n",
    "        print(X_tr.isna().sum()[X_tr.isna().sum() > 0])\n",
    "    \n",
    "    if X_v.isna().sum().sum() > 0:\n",
    "        print(\"\\nColumns with remaining NaNs in Val:\")\n",
    "        print(X_v.isna().sum()[X_v.isna().sum() > 0])\n",
    "    \n",
    "    if X_te.isna().sum().sum() > 0:\n",
    "        print(\"\\nColumns with remaining NaNs in Test:\")\n",
    "        print(X_te.isna().sum()[X_te.isna().sum() > 0])\n",
    "    \n",
    "    return X_tr, X_v, X_te\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e897d8",
   "metadata": {},
   "source": [
    "Chi2 test for feature importance in categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f6f48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestIndependence(X,y,var,alpha=0.05):        \n",
    "    dfObserved = pd.crosstab(y,X) \n",
    "    chi2, p, dof, expected = stats.chi2_contingency(dfObserved.values)\n",
    "    dfExpected = pd.DataFrame(expected, columns=dfObserved.columns, index = dfObserved.index)\n",
    "    if p<alpha:#if p<alpha we reject the null and there is a relationship so the var is important for prediction\n",
    "        result=\"{0} is IMPORTANT for Prediction\".format(var)#\n",
    "    else:\n",
    "        result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(var)#independent H0\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bf2af",
   "metadata": {},
   "source": [
    "Spearman correlation map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8845f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_heatmap(cor):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(data = cor, annot = True, cmap = plt.cm.Purples, fmt='.1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899e902",
   "metadata": {},
   "source": [
    "Lasso importance grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef868cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(coef,name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(6,8))\n",
    "    imp_coef.plot(kind = \"barh\", color='purple')\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74994862",
   "metadata": {},
   "source": [
    "Model evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19f608b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, X, y, split):\n",
    "    y_pred = model.predict(X)\n",
    "    return {\n",
    "        \"split\": split,\n",
    "        \"MAE\": mean_absolute_error(y, y_pred),\n",
    "        \"MedAE\": median_absolute_error(y, y_pred),\n",
    "        \"RMSE\": root_mean_squared_error(y, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y, y_pred),\n",
    "        \"R2\": r2_score(y, y_pred),\n",
    "    }\n",
    "\n",
    "def compute_metrics_log(model, X, y, split):\n",
    "    y_pred_log = model.predict(X)\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "    return {\n",
    "        \"split\": split,\n",
    "        \"MAE\": mean_absolute_error(y, y_pred),\n",
    "        \"MedAE\": median_absolute_error(y, y_pred),\n",
    "        \"RMSE\": root_mean_squared_error(y, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y, y_pred),\n",
    "        \"R2\": r2_score(y, y_pred),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c698579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X, y, scaler=None, model=None, fill_method=None):\n",
    "    \"\"\"\n",
    "    Train a model with optional preprocessing.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Features (will be copied to avoid modifying original)\n",
    "    - y: Target\n",
    "    - scaler: Scaler instance (e.g., StandardScaler()) or None for no scaling\n",
    "    - model: Model instance or None for LogisticRegression default\n",
    "    - fill_method: 'median', 'mean', or None for no filling\n",
    "    \n",
    "    Returns:\n",
    "    - model: Fitted model\n",
    "    - scaler: Fitted scaler (or None)\n",
    "    - fill_values: Dictionary of fill values (or None)\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying original data\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # Fill missing values - this function uses simple statistics from the training set but you can modify it to use more complex strategies\n",
    "    fill_values = None\n",
    "    if fill_method is not None:\n",
    "        if fill_method == 'function':\n",
    "            fill_values = impute_missing_values_hybrid(X_processed)\n",
    "        elif fill_method == 'mean':\n",
    "            fill_values = X_processed.mean()\n",
    "        X_processed = X_processed.fillna(fill_values)\n",
    "    \n",
    "    # Scale features\n",
    "    if scaler is not None:\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    \n",
    "    # Use provided model or create default\n",
    "    if model is None:\n",
    "        model = RandomForestRegressor()\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_processed, y)\n",
    "    \n",
    "    return model, scaler, fill_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7da33cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_rf_mae(X, y, model=None, scaler=None, fill_method=None):\n",
    "    \"\"\"\n",
    "    Avalia um modelo RandomForestRegressor usando o Mean Absolute Error (MAE).\n",
    "    \n",
    "    Esta versão ASSUME que X e y JÁ ESTÃO PRÉ-PROCESSADOS (X_val_final, y_val_final).\n",
    "    Os parâmetros 'scaler' e 'fill_values' são mantidos na assinatura para \n",
    "    compatibilidade, mas são ignorados no processamento interno.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features (Dados de validação já processados, e.g., X_val_final)\n",
    "    - y: Target (e.g., y_val)\n",
    "    - model: Modelo ajustado (Fitted RandomForestRegressor)\n",
    "    - scaler: Ignorado.\n",
    "    - fill_values: Ignorado.\n",
    "    \n",
    "    Returns:\n",
    "    - mae: Mean Absolute Error (Erro Absoluto Médio)\n",
    "    \"\"\"\n",
    "    # 1. Copia dos dados (mantido por segurança, embora não haja modificação)\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # 2. Imputação e Scaling SÃO IGNORADOS, pois os dados já estão processados\n",
    "    # if fill_values is not None: ...\n",
    "    # if scaler is not None: ...\n",
    "    \n",
    "    # 3. Fazer as previsões\n",
    "    y_pred = model.predict(X_processed)\n",
    "    \n",
    "    # 4. Calcular o MAE\n",
    "    # Nota: Assumindo que 'y' é o target na escala que se pretende (ex: log)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook functions.ipynb to python\n",
      "[NbConvertApp] Writing 34397 bytes to functions.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to python functions.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fall2526-DataMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
